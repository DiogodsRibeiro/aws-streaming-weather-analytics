# â›… Real Time Weather Analytics

> Pipeline de dados em tempo real para anÃ¡lise meteorolÃ³gica utilizando AWS Services

[![AWS](https://img.shields.io/badge/AWS-Cloud-orange?logo=amazon-aws)](https://aws.amazon.com/)
[![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ“‹ Ãndice

- [Sobre o Projeto](#sobre-o-projeto)
- [Arquitetura](#arquitetura)
- [Tecnologias Utilizadas](#tecnologias-utilizadas)
- [PrÃ©-requisitos](#prÃ©-requisitos)
- [InstalaÃ§Ã£o e ConfiguraÃ§Ã£o](#instalaÃ§Ã£o-e-configuraÃ§Ã£o)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Fluxo de Dados](#fluxo-de-dados)
- [Queries Athena](#queries-athena)
- [Monitoramento](#monitoramento)
- [Custos Estimados](#custos-estimados)
- [Troubleshooting](#troubleshooting)
- [Contribuindo](#contribuindo)

---

## ğŸ¯ Sobre o Projeto

Este projeto implementa uma soluÃ§Ã£o completa de **engenharia de dados em tempo real** para anÃ¡lise meteorolÃ³gica. O sistema consome dados de uma API de clima, processa informaÃ§Ãµes em tempo real, emite alertas automÃ¡ticos baseados em condiÃ§Ãµes climÃ¡ticas crÃ­ticas e armazena os dados de forma estruturada para anÃ¡lises ad-hoc.

### Principais Funcionalidades

- ğŸŒ¡ï¸ **IngestÃ£o em Tempo Real**: Coleta contÃ­nua de dados meteorolÃ³gicos via API Tomorrow.io
- ğŸ“¨ **Alertas Inteligentes**: NotificaÃ§Ãµes automÃ¡ticas por email quando condiÃ§Ãµes climÃ¡ticas crÃ­ticas sÃ£o detectadas
- ğŸ’¾ **Data Lake Estruturado**: Armazenamento em camadas (Raw â†’ Gold) com particionamento inteligente
- ğŸ”„ **ETL Automatizado**: TransformaÃ§Ã£o e estruturaÃ§Ã£o de dados com AWS Glue
- ğŸ“Š **AnÃ¡lise SQL**: Queries ad-hoc com Amazon Athena sobre dados em formato Parquet
- ğŸ“ˆ **Monitoramento**: Logs e mÃ©tricas em tempo real via CloudWatch

---

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        PRODUCER LAYER                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚ EventBridge  â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚   Lambda     â”‚                      â”‚
â”‚  â”‚  (Scheduler) â”‚         â”‚  Producer    â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                   â”‚                              â”‚
â”‚                                   v                              â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                          â”‚    Kinesis     â”‚                      â”‚
â”‚                          â”‚  Data Stream   â”‚                      â”‚
â”‚                          â”‚    (broker)    â”‚                      â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                   â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                               â”‚
                    v                               v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    REAL-TIME CONSUMER         â”‚   â”‚     BATCH CONSUMER           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Lambda                 â”‚  â”‚   â”‚  â”‚  Lambda                â”‚  â”‚
â”‚  â”‚  consumer_realtime      â”‚  â”‚   â”‚  â”‚  consumer_batch        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                 â”‚   â”‚             â”‚                â”‚
â”‚             v                 â”‚   â”‚             v                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Amazon SNS             â”‚  â”‚   â”‚  â”‚  S3 Bucket             â”‚  â”‚
â”‚  â”‚  (Email Alerts)         â”‚  â”‚   â”‚  â”‚  raw/year/month/day/   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                                   v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ETL LAYER (AWS GLUE)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚   Crawler    â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  Data        â”‚                      â”‚
â”‚  â”‚  raw_crawler â”‚         â”‚  Catalog     â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  (raw_db)    â”‚                      â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                  â”‚                              â”‚
â”‚                                  v                              â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                           â”‚  Glue Job    â”‚                      â”‚
â”‚                           â”‚ weather_job  â”‚                      â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                  â”‚                              â”‚
â”‚                                  v                              â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚                           â”‚  S3 Bucket   â”‚                      â”‚
â”‚                           â”‚  gold/       â”‚                      â”‚
â”‚                           â”‚  (Parquet)   â”‚                      â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                  â”‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚   Crawler    â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  Data        â”‚                      â”‚
â”‚  â”‚ gold_crawler â”‚         â”‚  Catalog     â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  (gold_db)   â”‚                      â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   v
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚   Athena     â”‚
                            â”‚  (SQL Queries)â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados Detalhado

1. **IngestÃ£o**: Lambda Producer coleta dados da API Tomorrow.io a cada intervalo programado
2. **Streaming**: Dados sÃ£o enviados para Kinesis Data Stream (broker)
3. **Processamento Dual**:
   - **Real-time**: Lambda detecta condiÃ§Ãµes crÃ­ticas e envia alertas via SNS
   - **Batch**: Lambda salva dados brutos no S3 (camada Raw) com particionamento temporal
4. **CatalogaÃ§Ã£o**: Crawler escaneia a camada Raw e registra schema no Glue Data Catalog
5. **TransformaÃ§Ã£o**: Glue Job processa dados (flatten JSON, conversÃµes) e salva em Parquet na camada Gold
6. **AnÃ¡lise**: Athena permite queries SQL sobre os dados estruturados

---

## ğŸ› ï¸ Tecnologias Utilizadas

### AWS Services

| ServiÃ§o | FunÃ§Ã£o | Detalhes |
|---------|--------|----------|
| **Lambda** | Processamento serverless | Producer, Consumer Real-time, Consumer Batch |
| **Kinesis Data Stream** | Message Broker | Stream "broker" para dados em tempo real |
| **S3** | Data Lake | Armazenamento em camadas Raw e Gold |
| **SNS** | NotificaÃ§Ãµes | Alertas por email sobre condiÃ§Ãµes climÃ¡ticas |
| **Glue** | ETL | Job de transformaÃ§Ã£o + Crawlers + Data Catalog |
| **Athena** | Query Engine | AnÃ¡lise SQL sobre dados no S3 |
| **IAM** | SeguranÃ§a | Roles e polÃ­ticas de acesso |
| **CloudWatch** | Observabilidade | Logs, mÃ©tricas e alarmes |
| **EventBridge** | Scheduler | Trigger periÃ³dico do Producer |

### Linguagens e Frameworks

- **Python 3.9+**: Lambdas e Glue Jobs
- **PySpark**: TransformaÃ§Ãµes no Glue
- **Boto3**: SDK AWS para Python
- **Requests**: HTTP client para API externa

### API Externa

- **Tomorrow.io Weather API**: Fonte de dados meteorolÃ³gicos em tempo real

---

## âœ… PrÃ©-requisitos

Antes de comeÃ§ar, vocÃª precisarÃ¡:

- [ ] Conta AWS ativa
- [ ] AWS CLI instalado e configurado
- [ ] Python 3.9+ instalado localmente
- [ ] Chave de API da Tomorrow.io ([criar conta aqui](https://app.tomorrow.io/home))
- [ ] Git instalado
- [ ] Editor de cÃ³digo (VSCode recomendado)

---

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. Clonar o RepositÃ³rio

```bash
git clone https://github.com/seu-usuario/real-time-weather-analytics.git
cd real-time-weather-analytics
```

### 2. Configurar AWS CLI

```bash
aws configure
# AWS Access Key ID: SUA_ACCESS_KEY
# AWS Secret Access Key: SUA_SECRET_KEY
# Default region name: us-east-2
# Default output format: json
```

### 3. Criar Kinesis Data Stream

```bash
aws kinesis create-stream \
    --stream-name broker \
    --shard-count 1 \
    --region us-east-2
```

### 4. Criar S3 Bucket

```bash
# Substitua NOME_UNICO por um nome Ãºnico
aws s3 mb s3://weatherrt2025 --region us-east-2
```

### 5. Criar IAM Roles

#### Role para Lambda Producer

```bash
aws iam create-role \
    --role-name lambda-producer-role \
    --assume-role-policy-document file://policies/lambda-trust-policy.json

aws iam attach-role-policy \
    --role-name lambda-producer-role \
    --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

# Criar polÃ­tica inline para Kinesis
aws iam put-role-policy \
    --role-name lambda-producer-role \
    --policy-name kinesis-put-record \
    --policy-document file://policies/kinesis-put-policy.json
```

#### Role para Lambda Consumer Real-time

```bash
aws iam create-role \
    --role-name lambda-consumer-realtime-role \
    --assume-role-policy-document file://policies/lambda-trust-policy.json

aws iam attach-role-policy \
    --role-name lambda-consumer-realtime-role \
    --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

# Adicionar permissÃµes SNS e Kinesis
aws iam put-role-policy \
    --role-name lambda-consumer-realtime-role \
    --policy-name sns-kinesis-access \
    --policy-document file://policies/sns-kinesis-policy.json
```

#### Role para Lambda Consumer Batch

```bash
aws iam create-role \
    --role-name lambda-consumer-batch-role \
    --assume-role-policy-document file://policies/lambda-trust-policy.json

aws iam attach-role-policy \
    --role-name lambda-consumer-batch-role \
    --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

# Adicionar permissÃµes S3 e Kinesis
aws iam put-role-policy \
    --role-name lambda-consumer-batch-role \
    --policy-name s3-kinesis-access \
    --policy-document file://policies/s3-kinesis-policy.json
```

#### Role para Glue

```bash
aws iam create-role \
    --role-name glue-etl-role \
    --assume-role-policy-document file://policies/glue-trust-policy.json

aws iam attach-role-policy \
    --role-name glue-etl-role \
    --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole

aws iam attach-role-policy \
    --role-name glue-etl-role \
    --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
```

### 6. Criar TÃ³pico SNS

```bash
# Criar tÃ³pico
aws sns create-topic --name snsalerta --region us-east-2

# Subscrever seu email
aws sns subscribe \
    --topic-arn arn:aws:sns:us-east-2:SEU_ACCOUNT_ID:snsalerta \
    --protocol email \
    --notification-endpoint seu-email@exemplo.com

# Confirme a inscriÃ§Ã£o no email recebido
```

### 7. Deploy das Lambdas

#### Lambda Producer

```bash
cd lambdas/producer
pip install -r requirements.txt -t .
zip -r producer.zip .

aws lambda create-function \
    --function-name producer \
    --runtime python3.9 \
    --role arn:aws:iam::SEU_ACCOUNT_ID:role/lambda-producer-role \
    --handler lambda_function.lambda_handler \
    --zip-file fileb://producer.zip \
    --timeout 30 \
    --memory-size 128 \
    --environment Variables={TOMORROW_API_KEY=SUA_API_KEY} \
    --region us-east-2
```

#### Lambda Consumer Real-time

```bash
cd ../consumer_realtime
zip -r consumer_realtime.zip lambda_function.py

aws lambda create-function \
    --function-name consumer_realtime \
    --runtime python3.9 \
    --role arn:aws:iam::SEU_ACCOUNT_ID:role/lambda-consumer-realtime-role \
    --handler lambda_function.lambda_handler \
    --zip-file fileb://consumer_realtime.zip \
    --timeout 60 \
    --memory-size 256 \
    --environment Variables={PRECIPITATION_PROBABILITY=70,RAIN_INTENSITY=5,WIND_GUST=10,WIND_SPEED=10} \
    --region us-east-2
```

#### Lambda Consumer Batch

```bash
cd ../consumer_batch
zip -r consumer_batch.zip lambda_function.py

aws lambda create-function \
    --function-name consumer_batch \
    --runtime python3.9 \
    --role arn:aws:iam::SEU_ACCOUNT_ID:role/lambda-consumer-batch-role \
    --handler lambda_function.lambda_handler \
    --zip-file fileb://consumer_batch.zip \
    --timeout 300 \
    --memory-size 512 \
    --environment Variables={BUCKET_NAME=weatherrt2025} \
    --region us-east-2
```

### 8. Configurar Triggers do Kinesis

```bash
# Trigger para Consumer Real-time
aws lambda create-event-source-mapping \
    --function-name consumer_realtime \
    --event-source-arn arn:aws:kinesis:us-east-2:SEU_ACCOUNT_ID:stream/broker \
    --starting-position LATEST \
    --batch-size 10

# Trigger para Consumer Batch
aws lambda create-event-source-mapping \
    --function-name consumer_batch \
    --event-source-arn arn:aws:kinesis:us-east-2:SEU_ACCOUNT_ID:stream/broker \
    --starting-position LATEST \
    --batch-size 100
```

### 9. Configurar EventBridge (Scheduler)

```bash
# Criar regra para executar Producer a cada 5 minutos
aws events put-rule \
    --name weather-producer-schedule \
    --schedule-expression "rate(5 minutes)"

# Adicionar permissÃ£o para EventBridge invocar Lambda
aws lambda add-permission \
    --function-name producer \
    --statement-id weather-producer-schedule \
    --action lambda:InvokeFunction \
    --principal events.amazonaws.com \
    --source-arn arn:aws:events:us-east-2:SEU_ACCOUNT_ID:rule/weather-producer-schedule

# Adicionar target (Lambda)
aws events put-targets \
    --rule weather-producer-schedule \
    --targets "Id"="1","Arn"="arn:aws:lambda:us-east-2:SEU_ACCOUNT_ID:function:producer"
```

### 10. Configurar AWS Glue

#### Criar Database

```bash
aws glue create-database \
    --database-input '{"Name":"raw_db","Description":"Raw weather data"}'

aws glue create-database \
    --database-input '{"Name":"gold_db","Description":"Processed weather data"}'
```

#### Criar Crawler Raw

```bash
aws glue create-crawler \
    --name raw_crawler \
    --role arn:aws:iam::SEU_ACCOUNT_ID:role/glue-etl-role \
    --database-name raw_db \
    --targets '{"S3Targets":[{"Path":"s3://weatherrt2025/raw/"}]}' \
    --schedule "cron(0 */6 * * ? *)"
```

#### Upload e Deploy do Glue Job

```bash
# Upload do script para S3
aws s3 cp glue/weather_job.py s3://weatherrt2025/scripts/

# Criar Glue Job
aws glue create-job \
    --name weather_job \
    --role arn:aws:iam::SEU_ACCOUNT_ID:role/glue-etl-role \
    --command Name=glueetl,ScriptLocation=s3://weatherrt2025/scripts/weather_job.py,PythonVersion=3 \
    --default-arguments '{"--job-language":"python","--enable-metrics":"","--enable-spark-ui":"true","--enable-continuous-cloudwatch-log":"true"}' \
    --max-capacity 2.0
```

#### Criar Crawler Gold

```bash
aws glue create-crawler \
    --name gold_crawler \
    --role arn:aws:iam::SEU_ACCOUNT_ID:role/glue-etl-role \
    --database-name gold_db \
    --targets '{"S3Targets":[{"Path":"s3://weatherrt2025/gold/"}]}' \
    --schedule "cron(0 */6 * * ? *)"
```

### 11. Executar Primeira IngestÃ£o

```bash
# Invocar Producer manualmente
aws lambda invoke \
    --function-name producer \
    --region us-east-2 \
    response.json

# Verificar resposta
cat response.json
```

---

## ğŸ“ Estrutura do Projeto

```
real-time-weather-analytics/
â”‚
â”œâ”€â”€ lambdas/
â”‚   â”œâ”€â”€ producer/
â”‚   â”‚   â”œâ”€â”€ lambda_function.py
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”œâ”€â”€ consumer_realtime/
â”‚   â”‚   â””â”€â”€ lambda_function.py
â”‚   â””â”€â”€ consumer_batch/
â”‚       â””â”€â”€ lambda_function.py
â”‚
â”œâ”€â”€ glue/
â”‚   â””â”€â”€ weather_job.py
â”‚
â”œâ”€â”€ policies/
â”‚   â”œâ”€â”€ lambda-trust-policy.json
â”‚   â”œâ”€â”€ glue-trust-policy.json
â”‚   â”œâ”€â”€ kinesis-put-policy.json
â”‚   â”œâ”€â”€ sns-kinesis-policy.json
â”‚   â””â”€â”€ s3-kinesis-policy.json
â”‚
â”œâ”€â”€ athena-queries/
â”‚   â”œâ”€â”€ views.sql
â”‚   â””â”€â”€ analytics.sql
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.png
â”‚   â””â”€â”€ deployment-guide.md
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸ”„ Fluxo de Dados

### 1. Producer (IngestÃ£o)

```python
# Lambda Producer - Executado a cada 5 minutos
API Tomorrow.io â†’ Lambda â†’ Kinesis Stream (broker)
```

**Dados coletados**:
- Temperatura atual e sensaÃ§Ã£o tÃ©rmica
- Umidade e ponto de orvalho
- PrecipitaÃ§Ã£o (probabilidade e intensidade)
- Vento (velocidade, rajadas, direÃ§Ã£o)
- Visibilidade e cobertura de nuvens
- Ãndice UV
- Coordenadas geogrÃ¡ficas
- Timestamp

### 2. Consumer Real-time (Alertas)

```python
# Trigger: Kinesis Stream
# Batch Size: 10 registros
# Processamento: < 1 segundo

Kinesis â†’ Lambda â†’ VerificaÃ§Ã£o de Thresholds â†’ SNS â†’ Email
```

**CondiÃ§Ãµes de Alerta**:
- Probabilidade de precipitaÃ§Ã£o â‰¥ 70%
- Intensidade de chuva â‰¥ 5 mm/h
- Rajadas de vento â‰¥ 10 m/s
- Velocidade do vento â‰¥ 10 m/s

### 3. Consumer Batch (Armazenamento)

```python
# Trigger: Kinesis Stream
# Batch Size: 100 registros
# Formato: JSON

Kinesis â†’ Lambda â†’ S3 (raw/year=YYYY/month=MM/day=DD/)
```

**Estrutura de Particionamento**:
```
s3://weatherrt2025/raw/
â”œâ”€â”€ year=2025/
â”‚   â”œâ”€â”€ month=11/
â”‚   â”‚   â”œâ”€â”€ day=06/
â”‚   â”‚   â”‚   â”œâ”€â”€ weather_data_2025-11-06T18:14:00.json
â”‚   â”‚   â”‚   â””â”€â”€ weather_data_2025-11-06T18:19:00.json
```

### 4. ETL (TransformaÃ§Ã£o)

```python
# Glue Job - Executado apÃ³s Crawler Raw
# Input: JSON (Raw Layer)
# Output: Parquet (Gold Layer)

Raw JSON â†’ Glue Job â†’ Flatten + Type Casting â†’ Parquet (particionado)
```

**TransformaÃ§Ãµes realizadas**:
- Flatten de JSON aninhado
- ConversÃ£o de tipos de dados
- ExtraÃ§Ã£o de year/month/day do timestamp
- RemoÃ§Ã£o de campos desnecessÃ¡rios
- CompressÃ£o em formato Parquet

---

## ğŸ“Š Queries Athena

### Setup Inicial

```sql
-- Database jÃ¡ criado pelos crawlers: gold_db

-- Reparar partiÃ§Ãµes (executar apÃ³s cada ETL)
MSCK REPAIR TABLE gold_db.weather_data;
```

### Views Sugeridas

#### 1. CondiÃ§Ãµes Atuais

```sql
CREATE OR REPLACE VIEW gold_db.current_conditions AS
SELECT 
    time,
    temperature,
    temperatureapparent,
    humidity,
    precipitationprobability,
    windspeed,
    windgust,
    weathercode,
    visibility
FROM gold_db.weather_data
WHERE year = YEAR(CURRENT_DATE)
  AND month = MONTH(CURRENT_DATE)
  AND day = DAY_OF_MONTH(CURRENT_DATE)
ORDER BY time DESC
LIMIT 1;
```

#### 2. EstatÃ­sticas DiÃ¡rias

```sql
CREATE OR REPLACE VIEW gold_db.daily_statistics AS
SELECT 
    CAST(CONCAT(CAST(year AS VARCHAR), '-', 
                LPAD(CAST(month AS VARCHAR), 2, '0'), '-', 
                LPAD(CAST(day AS VARCHAR), 2, '0')) AS DATE) as date,
    ROUND(AVG(temperature), 2) as avg_temp,
    ROUND(MAX(temperature), 2) as max_temp,
    ROUND(MIN(temperature), 2) as min_temp,
    ROUND(AVG(humidity), 2) as avg_humidity,
    ROUND(AVG(windspeed), 2) as avg_wind_speed,
    ROUND(MAX(windgust), 2) as max_wind_gust,
    ROUND(SUM(rainintensity), 2) as total_rain,
    COUNT(*) as measurements
FROM gold_db.weather_data
GROUP BY year, month, day
ORDER BY year DESC, month DESC, day DESC;
```

#### 3. Alertas HistÃ³ricos

```sql
CREATE OR REPLACE VIEW gold_db.historical_alerts AS
SELECT 
    time,
    temperature,
    precipitationprobability,
    rainintensity,
    windspeed,
    windgust,
    CASE 
        WHEN precipitationprobability >= 70 THEN 'High Precipitation Risk'
        WHEN rainintensity >= 5 THEN 'Heavy Rain'
        WHEN windgust >= 10 THEN 'Strong Wind Gusts'
        WHEN windspeed >= 10 THEN 'High Wind Speed'
    END as alert_type
FROM gold_db.weather_data
WHERE precipitationprobability >= 70
   OR rainintensity >= 5
   OR windgust >= 10
   OR windspeed >= 10
ORDER BY time DESC;
```

#### 4. AnÃ¡lise Semanal

```sql
CREATE OR REPLACE VIEW gold_db.weekly_trends AS
SELECT 
    DATE_TRUNC('week', 
        CAST(CONCAT(CAST(year AS VARCHAR), '-', 
                    LPAD(CAST(month AS VARCHAR), 2, '0'), '-', 
                    LPAD(CAST(day AS VARCHAR), 2, '0')) AS DATE)
    ) as week_start,
    ROUND(AVG(temperature), 2) as avg_temp,
    ROUND(AVG(humidity), 2) as avg_humidity,
    ROUND(AVG(windspeed), 2) as avg_wind,
    ROUND(SUM(rainintensity), 2) as total_rain,
    COUNT(*) as measurements
FROM gold_db.weather_data
GROUP BY DATE_TRUNC('week', 
    CAST(CONCAT(CAST(year AS VARCHAR), '-', 
                LPAD(CAST(month AS VARCHAR), 2, '0'), '-', 
                LPAD(CAST(day AS VARCHAR), 2, '0')) AS DATE))
ORDER BY week_start DESC;
```

#### 5. CondiÃ§Ãµes Extremas

```sql
CREATE OR REPLACE VIEW gold_db.extreme_conditions AS
SELECT 
    'Highest Temperature' as condition_type,
    time,
    temperature as value,
    'Celsius' as unit
FROM gold_db.weather_data
WHERE temperature = (SELECT MAX(temperature) FROM gold_db.weather_data)

UNION ALL

SELECT 
    'Lowest Temperature',
    time,
    temperature,
    'Celsius'
FROM gold_db.weather_data
WHERE temperature = (SELECT MIN(temperature) FROM gold_db.weather_data)

UNION ALL

SELECT 
    'Highest Wind Gust',
    time,
    windgust,
    'm/s'
FROM gold_db.weather_data
WHERE windgust = (SELECT MAX(windgust) FROM gold_db.weather_data)

UNION ALL

SELECT 
    'Heaviest Rain',
    time,
    rainintensity,
    'mm/h'
FROM gold_db.weather_data
WHERE rainintensity = (SELECT MAX(rainintensity) FROM gold_db.weather_data);
```

#### 6. AnÃ¡lise de Conforto TÃ©rmico

```sql
CREATE OR REPLACE VIEW gold_db.thermal_comfort AS
SELECT 
    time,
    temperature,
    temperatureapparent,
    humidity,
    ROUND(temperatureapparent - temperature, 2) as comfort_index,
    CASE 
        WHEN temperatureapparent - temperature > 5 THEN 'Muito DesconfortÃ¡vel'
        WHEN temperatureapparent - temperature > 2 THEN 'DesconfortÃ¡vel'
        WHEN ABS(temperatureapparent - temperature) <= 2 THEN 'ConfortÃ¡vel'
        ELSE 'Fresco'
    END as comfort_level
FROM gold_db.weather_data
ORDER BY time DESC;
```

### Queries AnalÃ­ticas

```sql
-- 1. DistribuiÃ§Ã£o de temperatura por hora do dia
SELECT 
    EXTRACT(HOUR FROM time) as hour_of_day,
    ROUND(AVG(temperature), 2) as avg_temp,
    ROUND(MIN(temperature), 2) as min_temp,
    ROUND(MAX(temperature), 2) as max_temp
FROM gold_db.weather_data
GROUP BY EXTRACT(HOUR FROM time)
ORDER BY hour_of_day;

-- 2. Dias com maior probabilidade de chuva
SELECT 
    CAST(CONCAT(CAST(year AS VARCHAR), '-', 
                LPAD(CAST(month AS VARCHAR), 2, '0'), '-', 
                LPAD(CAST(day AS VARCHAR), 2, '0')) AS DATE) as date,
    ROUND(AVG(precipitationprobability), 2) as avg_rain_probability,
    ROUND(SUM(rainintensity), 2) as total_rain
FROM gold_db.weather_data
GROUP BY year, month, day
HAVING AVG(precipitationprobability) > 50
ORDER BY avg_rain_probability DESC;

-- 3. CorrelaÃ§Ã£o vento e temperatura
SELECT 
    CASE 
        WHEN temperature < 10 THEN 'Frio'
        WHEN temperature BETWEEN 10 AND 20 THEN 'Ameno'
        WHEN temperature BETWEEN 20 AND 30 THEN 'Quente'
        ELSE 'Muito Quente'
    END as temp_range,
    ROUND(AVG(windspeed), 2) as avg_wind_speed,
    COUNT(*) as occurrences
FROM gold_db.weather_data
GROUP BY CASE 
    WHEN temperature < 10 THEN 'Frio'
    WHEN temperature BETWEEN 10 AND 20 THEN 'Ameno'
    WHEN temperature BETWEEN 20 AND 30 THEN 'Quente'
    ELSE 'Muito Quente'
END;

-- 4. AnÃ¡lise de visibilidade
SELECT 
    CAST(CONCAT(CAST(year AS VARCHAR), '-', 
                LPAD(CAST(month AS VARCHAR), 2, '0'), '-', 
                LPAD(CAST(day AS VARCHAR), 2, '0')) AS DATE) as date,
    ROUND(AVG(visibility), 2) as avg_visibility,
    ROUND(MIN(visibility), 2) as min_visibility,
    COUNT(CASE WHEN visibility < 5 THEN 1 END) as low_visibility_count
FROM gold_db.weather_data
GROUP BY year, month, day
ORDER BY avg_visibility ASC
LIMIT 10;
```

---

## ğŸ“ˆ Monitoramento

### CloudWatch Logs

Cada Lambda gera logs automÃ¡ticos:

```
/aws/lambda/producer
/aws/lambda/consumer_realtime
/aws/lambda/consumer_batch
```

### MÃ©tricas do Kinesis

```bash
# Visualizar mÃ©tricas do stream
aws cloudwatch get-metric-statistics \
    --namespace AWS/Kinesis \
    --metric-name IncomingRecords \
    --dimensions Name=StreamName,Value=broker \
    --start-time 2025-11-06T00:00:00Z \
    --end-time 2025-11-06T23:59:59Z \
    --period 3600 \
    --statistics Sum
```

### Alarmes Recomendados

```bash
# Alarme: Lambda com muitos erros
aws cloudwatch put-metric-alarm \
    --alarm-name lambda-producer-errors \
    --alarm-description "Alert when producer lambda has too many errors" \
    --metric-name Errors \
    --namespace AWS/Lambda \
    --statistic Sum \
    --period 300 \
    --threshold 5 \
    --comparison-operator GreaterThanThreshold \
    --evaluation-periods 1 \
    --dimensions Name=FunctionName,Value=producer

# Alarme: Kinesis com alto iterator age
aws cloudwatch put-metric-alarm \
    --alarm-name kinesis-iterator-age \
    --alarm-description "Alert when Kinesis iterator age is too high" \
    --metric-name GetRecords.IteratorAgeMilliseconds \
    --namespace AWS/Kinesis \
    --statistic Average \
    --period 300 \
    --threshold 60000 \
    --comparison-operator GreaterThanThreshold \
    --evaluation-periods 2 \
    --dimensions Name=StreamName,Value=broker
```

### Dashboard CloudWatch

Crie um dashboard customizado:

1. Acesse CloudWatch Console
2. Dashboards â†’ Create dashboard
3. Adicione widgets para:
   - Lambda invocations (Producer, Consumers)
   - Lambda errors e duraÃ§Ã£o
   - Kinesis incoming/outgoing records
   - S3 bucket size (Raw e Gold)
   - SNS published messages

---

## ğŸ’° Custos Estimados

### CÃ¡lculo Mensal (estimativa para uso 24/7)

| ServiÃ§o | Uso | Custo Mensal (USD) |
|---------|-----|-------------------|
| **Lambda Producer** | 8.640 execuÃ§Ãµes/mÃªs (5 min), 128MB, 5s | ~$0.10 |
| **Lambda Consumers** | 8.640 execuÃ§Ãµes/mÃªs cada, 256-512MB | ~$0.50 |
| **Kinesis Data Stream** | 1 shard, 8.640 registros/mÃªs | ~$15.00 |
| **S3 Storage** | 10GB Raw + 5GB Gold | ~$0.35 |
| **Glue Crawlers** | 4 execuÃ§Ãµes/dia, 2 crawlers | ~$0.88 |
| **Glue ETL Job** | 4 execuÃ§Ãµes/dia, 2 DPUs, 5 min | ~$4.00 |
| **Athena** | 100GB scanned/mÃªs | ~$0.50 |
| **SNS** | 1.000 notificaÃ§Ãµes/mÃªs | ~$0.50 |
| **CloudWatch** | Logs padrÃ£o | ~$2.00 |
| **TOTAL ESTIMADO** | | **~$23.83/mÃªs** |

> **Nota**: Custos podem variar conforme regiÃ£o e uso real. Use a [AWS Pricing Calculator](https://calculator.aws) para estimativas precisas.

### OtimizaÃ§Ãµes de Custo

- âœ… Use S3 Intelligent-Tiering para dados antigos
- âœ… Configure lifecycle policies para mover dados para Glacier apÃ³s 90 dias
- âœ… Reduza frequÃªncia do Producer se nÃ£o precisar de dados a cada 5 minutos
- âœ… Use Athena com compressÃ£o Parquet (jÃ¡ implementado)
- âœ… Monitore e ajuste memory/timeout das Lambdas

---

## ğŸ”§ Troubleshooting

### Problema: Lambda nÃ£o consegue acessar Kinesis

**Erro**: `Cannot access stream arn:aws:kinesis...`

**SoluÃ§Ã£o**:
```bash
# Adicionar polÃ­tica inline ao role da Lambda
aws iam put-role-policy \
    --role-name NOME_DO_ROLE \
    --policy-name KinesisAccess \
    --policy-document '{
        "Version": "2012-10-17",
        "Statement": [{
            "Effect": "Allow",
            "Action": [
                "kinesis:GetRecords",
                "kinesis:GetShardIterator",
                "kinesis:DescribeStream",
                "kinesis:DescribeStreamSummary",
                "kinesis:ListShards",
                "kinesis:SubscribeToShard"
            ],
            "Resource": "arn:aws:kinesis:us-east-2:*:stream/broker"
        }]
    }'
```

### Problema: SNS nÃ£o envia emails

**VerificaÃ§Ãµes**:
1. Confirme inscriÃ§Ã£o no email
2. Verifique spam/lixeira
3. Teste publicaÃ§Ã£o manual:
```bash
aws sns publish \
    --topic-arn arn:aws:sns:us-east-2:SEU_ACCOUNT_ID:snsalerta \
    --message "Test message"
```

### Problema: Crawler nÃ£o detecta partiÃ§Ãµes

**SoluÃ§Ã£o**:
```bash
# Executar MSCK REPAIR TABLE no Athena
MSCK REPAIR TABLE gold_db.weather_data;

# Ou reconfigurar crawler para detectar partiÃ§Ãµes
aws glue update-crawler \
    --name gold_crawler \
    --configuration '{"Version":1.0,"CrawlerOutput":{"Partitions":{"AddOrUpdateBehavior":"InheritFromTable"}}}'
```

### Problema: Glue Job falha

**VerificaÃ§Ãµes**:
1. Verifique logs no CloudWatch: `/aws-glue/jobs/error` e `/aws-glue/jobs/output`
2. Confirme que schema do Raw existe no Catalog
3. Teste job com pequeno dataset primeiro

### Problema: Athena retorna "TABLE NOT FOUND"

**SoluÃ§Ã£o**:
```sql
-- Listar databases
SHOW DATABASES;

-- Listar tabelas
SHOW TABLES IN gold_db;

-- Se nÃ£o existir, execute o crawler
```

---

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Para contribuir:

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/NovaFuncionalidade`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/NovaFuncionalidade`)
5. Abra um Pull Request

### Boas PrÃ¡ticas

- Siga PEP 8 para cÃ³digo Python
- Adicione testes unitÃ¡rios
- Documente novas funcionalidades
- Mantenha o README atualizado

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---

## ğŸ‘¨â€ğŸ’» Autor

**Seu Nome**
- LinkedIn: [seu-perfil](https://linkedin.com/in/seu-perfil)
- GitHub: [@seu-usuario](https://github.com/seu-usuario)
- Email: seu-email@exemplo.com

---

## ğŸ™ Agradecimentos

- [Tomorrow.io](https://tomorrow.io) pela API de dados meteorolÃ³gicos
- AWS pela infraestrutura cloud
- Comunidade de Engenharia de Dados

---

## ğŸ“š Recursos Adicionais

- [AWS Lambda Best Practices](https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html)
- [Kinesis Data Streams Guide](https://docs.aws.amazon.com/streams/latest/dev/introduction.html)
- [AWS Glue ETL Best Practices](https://docs.aws.amazon.com/glue/latest/dg/best-practices.html)
- [Athena Performance Tuning](https://docs.aws.amazon.com/athena/latest/ug/performance-tuning.html)

---

<div align="center">

**â­ Se este projeto foi Ãºtil, considere dar uma estrela!**

</div>
